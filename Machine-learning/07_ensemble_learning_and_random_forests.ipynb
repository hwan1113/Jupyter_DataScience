{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting=\"hard\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "# soft voting with probability\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting=\"hard\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples = 100, bootstrap=True, n_jobs=-1\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs= -1, oob_score=True\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "# each train(x 500) makes oob score\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42458101, 0.57541899],\n",
       "       [0.36094675, 0.63905325],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07909605, 0.92090395],\n",
       "       [0.36144578, 0.63855422],\n",
       "       [0.01578947, 0.98421053],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [0.79039301, 0.20960699],\n",
       "       [0.00578035, 0.99421965],\n",
       "       [0.71052632, 0.28947368],\n",
       "       [0.7967033 , 0.2032967 ],\n",
       "       [0.96111111, 0.03888889],\n",
       "       [0.06315789, 0.93684211],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.93010753, 0.06989247],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00591716, 0.99408284],\n",
       "       [0.33908046, 0.66091954],\n",
       "       [0.92146597, 0.07853403],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.64390244, 0.35609756],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14444444, 0.85555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.34705882, 0.65294118],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26946108, 0.73053892],\n",
       "       [0.34972678, 0.65027322],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02197802, 0.97802198],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.86826347, 0.13173653],\n",
       "       [0.9673913 , 0.0326087 ],\n",
       "       [0.98275862, 0.01724138],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06896552, 0.93103448],\n",
       "       [0.99484536, 0.00515464],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86315789, 0.13684211],\n",
       "       [0.33980583, 0.66019417],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66257669, 0.33742331],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83522727, 0.16477273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.64835165, 0.35164835],\n",
       "       [0.13872832, 0.86127168],\n",
       "       [0.63535912, 0.36464088],\n",
       "       [0.91860465, 0.08139535],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22797927, 0.77202073],\n",
       "       [0.87830688, 0.12169312],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.05641026, 0.94358974],\n",
       "       [0.03365385, 0.96634615],\n",
       "       [0.29347826, 0.70652174],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.84974093, 0.15025907],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.28877005, 0.71122995],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94767442, 0.05232558],\n",
       "       [0.78235294, 0.21764706],\n",
       "       [0.00595238, 0.99404762],\n",
       "       [1.        , 0.        ],\n",
       "       [0.19230769, 0.80769231],\n",
       "       [0.62311558, 0.37688442],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03314917, 0.96685083],\n",
       "       [0.49112426, 0.50887574],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02793296, 0.97206704],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29878049, 0.70121951],\n",
       "       [0.51685393, 0.48314607],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.98895028, 0.01104972],\n",
       "       [0.29142857, 0.70857143],\n",
       "       [0.89189189, 0.10810811],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.79255319, 0.20744681],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02083333, 0.97916667],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98445596, 0.01554404],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94444444, 0.05555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02298851, 0.97701149],\n",
       "       [0.22279793, 0.77720207],\n",
       "       [0.94021739, 0.05978261],\n",
       "       [0.31764706, 0.68235294],\n",
       "       [0.98974359, 0.01025641],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.7032967 , 0.2967033 ],\n",
       "       [0.35714286, 0.64285714],\n",
       "       [0.35672515, 0.64327485],\n",
       "       [0.88601036, 0.11398964],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.07514451, 0.92485549],\n",
       "       [0.74731183, 0.25268817],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01162791, 0.98837209],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91351351, 0.08648649],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.39795918, 0.60204082],\n",
       "       [0.26666667, 0.73333333],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.28272251, 0.71727749],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98984772, 0.01015228],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00460829, 0.99539171],\n",
       "       [0.67039106, 0.32960894],\n",
       "       [0.90697674, 0.09302326],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09090909, 0.90909091],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03389831, 0.96610169],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0201005 , 0.9798995 ],\n",
       "       [0.9950495 , 0.0049505 ],\n",
       "       [0.93532338, 0.06467662],\n",
       "       [0.73076923, 0.26923077],\n",
       "       [0.57309942, 0.42690058],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13636364, 0.86363636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96236559, 0.03763441],\n",
       "       [0.98795181, 0.01204819],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42424242, 0.57575758],\n",
       "       [0.83240223, 0.16759777],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24861878, 0.75138122],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97560976, 0.02439024],\n",
       "       [0.82608696, 0.17391304],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0821256 , 0.9178744 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02617801, 0.97382199],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06024096, 0.93975904],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76884422, 0.23115578],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87634409, 0.12365591],\n",
       "       [0.98536585, 0.01463415],\n",
       "       [0.15544041, 0.84455959],\n",
       "       [0.225     , 0.775     ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.19161677, 0.80838323],\n",
       "       [0.94054054, 0.05945946],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.51098901, 0.48901099],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08602151, 0.91397849],\n",
       "       [0.06703911, 0.93296089],\n",
       "       [0.98181818, 0.01818182],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.37566138, 0.62433862],\n",
       "       [0.0959596 , 0.9040404 ],\n",
       "       [0.4742268 , 0.5257732 ],\n",
       "       [0.55376344, 0.44623656],\n",
       "       [0.01149425, 0.98850575],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.58201058, 0.41798942],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.22619048, 0.77380952],\n",
       "       [0.80412371, 0.19587629],\n",
       "       [0.05780347, 0.94219653],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84269663, 0.15730337],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.10555556, 0.89444444],\n",
       "       [0.00943396, 0.99056604],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91071429, 0.08928571],\n",
       "       [0.13812155, 0.86187845],\n",
       "       [0.94708995, 0.05291005],\n",
       "       [0.01136364, 0.98863636],\n",
       "       [0.57471264, 0.42528736],\n",
       "       [0.08938547, 0.91061453],\n",
       "       [0.98830409, 0.01169591],\n",
       "       [0.81818182, 0.18181818],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94350282, 0.05649718],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27272727, 0.72727273],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.85416667, 0.14583333],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.9197861 , 0.0802139 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.67875648, 0.32124352],\n",
       "       [0.49717514, 0.50282486],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87134503, 0.12865497],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74157303, 0.25842697],\n",
       "       [0.12972973, 0.87027027],\n",
       "       [0.45180723, 0.54819277],\n",
       "       [0.22395833, 0.77604167],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90673575, 0.09326425],\n",
       "       [0.80666667, 0.19333333],\n",
       "       [0.01092896, 0.98907104],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01554404, 0.98445596],\n",
       "       [0.94416244, 0.05583756],\n",
       "       [0.96774194, 0.03225806],\n",
       "       [1.        , 0.        ],\n",
       "       [0.51648352, 0.48351648],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95027624, 0.04972376],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02873563, 0.97126437],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01162791, 0.98837209],\n",
       "       [1.        , 0.        ],\n",
       "       [0.09836066, 0.90163934],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01025641, 0.98974359],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38857143, 0.61142857],\n",
       "       [0.0591133 , 0.9408867 ],\n",
       "       [0.25988701, 0.74011299],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18421053, 0.81578947],\n",
       "       [0.98895028, 0.01104972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95121951, 0.04878049],\n",
       "       [0.27027027, 0.72972973],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00990099, 0.99009901],\n",
       "       [0.98857143, 0.01142857],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03389831, 0.96610169],\n",
       "       [0.98941799, 0.01058201],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02515723, 0.97484277],\n",
       "       [0.58378378, 0.41621622]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows what each sample would have gotten if it were out of the bag sample and predicted against the trained data\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10035667189208082\n",
      "sepal width (cm) 0.022627277927040675\n",
      "petal length (cm) 0.45849400484788505\n",
      "petal width (cm) 0.4185220453329934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.140625   -0.07317073  0.140625   -0.07317073 -0.07317073  0.140625\n",
      "  0.140625   -0.07317073 -0.07317073  0.140625   -0.859375   -0.859375\n",
      "  0.140625   -0.07317073  0.140625    0.140625    0.140625   -0.07317073\n",
      "  0.140625    0.140625    0.140625    0.          0.140625    0.140625\n",
      " -0.07317073  0.          0.140625   -0.07317073 -0.07317073  0.140625\n",
      "  0.140625    0.140625    0.140625   -0.07317073 -0.07317073  0.140625\n",
      "  0.140625   -0.07317073 -0.07317073 -0.07317073  0.140625   -0.07317073\n",
      "  0.          0.         -0.859375   -0.07317073 -0.859375   -0.07317073\n",
      "  0.140625   -0.859375   -0.07317073 -0.07317073 -0.07317073  0.140625\n",
      " -0.07317073 -0.07317073 -0.07317073 -0.859375   -0.07317073 -0.07317073\n",
      "  0.140625    0.140625    0.140625    0.140625   -0.07317073 -0.07317073\n",
      "  0.140625   -0.07317073  0.          0.140625    0.140625    0.140625\n",
      "  0.140625    0.92682927  0.140625    0.140625    0.92682927 -0.07317073\n",
      "  0.140625    0.140625   -0.07317073 -0.07317073  0.         -0.07317073\n",
      "  0.140625   -0.07317073 -0.07317073  0.140625    0.140625    0.92682927\n",
      "  0.140625    0.140625    0.140625    0.140625    0.140625    0.140625\n",
      " -0.859375   -0.07317073  0.140625   -0.859375    0.          0.140625\n",
      " -0.07317073  0.140625    0.140625   -0.07317073  0.140625    0.\n",
      "  0.140625   -0.07317073 -0.07317073  0.140625    0.140625   -0.859375\n",
      "  0.140625    0.140625    0.          0.         -0.07317073 -0.859375\n",
      " -0.07317073  0.          0.140625   -0.07317073 -0.07317073 -0.07317073\n",
      "  0.140625   -0.07317073 -0.07317073  0.140625   -0.859375   -0.07317073\n",
      "  0.140625    0.92682927 -0.07317073 -0.07317073 -0.07317073 -0.07317073\n",
      " -0.859375    0.140625   -0.859375    0.          0.         -0.07317073\n",
      "  0.140625    0.140625   -0.07317073  0.          0.140625    0.140625\n",
      " -0.859375   -0.07317073 -0.859375   -0.07317073  0.140625    0.140625\n",
      " -0.859375   -0.07317073  0.140625   -0.859375   -0.07317073  0.140625\n",
      "  0.140625   -0.07317073  0.140625    0.140625   -0.859375   -0.07317073\n",
      "  0.140625   -0.07317073 -0.859375   -0.07317073  0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073  0.140625   -0.07317073 -0.07317073  0.140625\n",
      "  0.140625   -0.07317073  0.140625    0.140625    0.140625   -0.07317073\n",
      "  0.140625   -0.07317073 -0.07317073 -0.07317073  0.          0.140625\n",
      "  0.140625    0.140625   -0.07317073  0.         -0.07317073 -0.07317073\n",
      " -0.07317073  0.140625    0.140625   -0.07317073  0.         -0.07317073\n",
      " -0.07317073  0.         -0.07317073 -0.859375    0.140625    0.140625\n",
      "  0.92682927 -0.07317073  0.140625    0.140625    0.92682927  0.140625\n",
      "  0.140625   -0.07317073  0.140625    0.          0.140625    0.140625\n",
      " -0.07317073 -0.07317073 -0.07317073  0.140625    0.          0.140625\n",
      "  0.140625   -0.07317073 -0.07317073 -0.07317073  0.140625   -0.07317073\n",
      "  0.140625    0.140625    0.140625   -0.07317073 -0.07317073 -0.07317073\n",
      "  0.140625   -0.07317073  0.140625   -0.859375    0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073  0.140625   -0.07317073  0.140625    0.140625\n",
      "  0.140625    0.          0.140625    0.140625   -0.07317073  0.140625\n",
      " -0.07317073  0.140625    0.140625   -0.07317073  0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073  0.140625   -0.07317073  0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073  0.          0.140625    0.140625    0.140625\n",
      "  0.140625    0.140625    0.140625   -0.07317073  0.140625   -0.07317073\n",
      "  0.140625    0.         -0.07317073  0.140625    0.140625    0.140625\n",
      "  0.140625    0.140625    0.140625    0.140625    0.140625    0.140625\n",
      " -0.07317073 -0.07317073  0.140625    0.140625    0.140625    0.140625\n",
      "  0.140625    0.          0.92682927  0.         -0.07317073 -0.07317073\n",
      "  0.92682927 -0.07317073 -0.07317073 -0.07317073  0.140625   -0.07317073\n",
      "  0.140625    0.140625   -0.07317073  0.140625    0.140625   -0.07317073\n",
      "  0.140625    0.140625   -0.07317073  0.140625   -0.07317073  0.140625\n",
      " -0.07317073  0.140625   -0.07317073  0.         -0.07317073  0.140625\n",
      " -0.07317073 -0.859375   -0.07317073 -0.859375   -0.07317073 -0.07317073\n",
      "  0.140625    0.140625    0.         -0.07317073 -0.07317073 -0.07317073\n",
      "  0.          0.140625   -0.07317073 -0.859375    0.          0.140625\n",
      " -0.07317073  0.140625    0.140625    0.140625   -0.859375    0.140625\n",
      "  0.140625   -0.07317073  0.          0.140625   -0.07317073 -0.07317073\n",
      " -0.07317073  0.140625   -0.07317073 -0.07317073 -0.07317073 -0.859375\n",
      "  0.140625    0.140625   -0.07317073  0.140625   -0.07317073  0.140625\n",
      "  0.          0.92682927  0.92682927 -0.07317073  0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073 -0.07317073  0.140625   -0.859375    0.140625\n",
      "  0.92682927 -0.07317073  0.140625   -0.859375   -0.07317073 -0.07317073\n",
      "  0.140625    0.          0.         -0.07317073  0.140625    0.140625\n",
      "  0.140625    0.140625    0.         -0.07317073 -0.07317073  0.140625\n",
      "  0.140625    0.92682927  0.140625    0.          0.140625    0.140625\n",
      "  0.92682927 -0.07317073 -0.07317073 -0.859375   -0.859375    0.140625\n",
      " -0.07317073  0.140625    0.140625   -0.859375   -0.07317073  0.140625\n",
      " -0.07317073  0.140625    0.140625   -0.07317073  0.140625   -0.859375\n",
      " -0.07317073 -0.07317073  0.140625    0.140625    0.140625   -0.07317073\n",
      "  0.140625    0.140625   -0.07317073  0.140625    0.140625   -0.07317073\n",
      " -0.07317073  0.140625   -0.07317073  0.140625    0.140625   -0.07317073\n",
      "  0.140625    0.         -0.07317073  0.140625    0.          0.140625\n",
      " -0.07317073 -0.07317073  0.140625   -0.07317073  0.140625   -0.07317073\n",
      "  0.140625   -0.07317073  0.         -0.07317073 -0.07317073 -0.07317073\n",
      " -0.07317073  0.140625   -0.859375    0.92682927 -0.859375    0.140625\n",
      "  0.140625   -0.07317073  0.140625    0.140625   -0.07317073  0.140625\n",
      "  0.92682927  0.140625    0.140625    0.140625    0.140625   -0.07317073\n",
      " -0.07317073 -0.07317073  0.140625   -0.07317073  0.140625   -0.859375\n",
      "  0.140625    0.140625   -0.07317073  0.140625   -0.07317073  0.\n",
      " -0.859375   -0.07317073 -0.07317073 -0.07317073  0.140625    0.140625\n",
      "  0.140625   -0.07317073]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7362266 ,  0.1015663 ,  0.1015663 ,  0.85124217,  1.02257171,\n",
       "        0.93722015,  0.1015663 ,  0.1015663 , -0.1231484 ,  0.1015663 ,\n",
       "        1.0457984 ,  0.1015663 ,  0.93722015,  0.7658906 ,  0.89856769,\n",
       "        0.1015663 ,  0.1015663 ,  0.89856769,  0.89856769,  0.1015663 ,\n",
       "        0.1015663 ,  1.19073666,  0.7362266 , -0.1231484 ,  0.1015663 ,\n",
       "        0.1015663 ,  0.7362266 ,  0.1015663 ,  0.96044684,  0.1015663 ,\n",
       "        0.96044684,  1.0457984 , -0.06976324,  0.1015663 ,  0.85124217,\n",
       "        0.56489706,  0.1015663 ,  0.85124217,  0.85124217,  0.96044684,\n",
       "        0.26390739,  0.93722015,  0.56489706,  0.1015663 ,  0.1015663 ,\n",
       "        0.1015663 ,  0.7658906 ,  0.7362266 ,  1.02257171,  0.7362266 ,\n",
       "        0.7362266 ,  1.0457984 ,  0.1015663 ,  0.1015663 ,  0.7362266 ,\n",
       "        0.1015663 ,  0.7362266 ,  0.85124217,  0.1015663 ,  0.96044684,\n",
       "        0.1015663 ,  0.7658906 ,  0.89856769,  0.1015663 ,  0.89856769,\n",
       "       -0.06976324, -0.06976324,  0.56489706,  0.1015663 ,  1.19073666,\n",
       "        0.56489706,  0.1015663 ,  0.7362266 ,  0.7658906 ,  0.1015663 ,\n",
       "        0.1015663 ,  1.0457984 ,  0.85124217,  0.1015663 ,  0.1015663 ,\n",
       "        0.89856769,  1.19073666,  0.88777057,  0.56489706,  0.96044684,\n",
       "        0.7658906 ,  0.7658906 ,  0.1015663 ,  0.85124217,  1.02257171,\n",
       "        1.0457984 ,  0.1015663 ,  0.1015663 ,  0.1015663 ,  0.1015663 ,\n",
       "        0.96044684,  0.1015663 ,  0.56489706,  0.1015663 ,  0.85124217,\n",
       "        0.1015663 ,  0.89856769,  0.96044684, -0.06976324, -0.06976324,\n",
       "        0.1015663 ,  0.07784515, -0.06976324,  0.56489706,  0.89856769,\n",
       "        0.26390739, -0.06976324,  0.1015663 ,  1.19073666,  0.7362266 ,\n",
       "        0.1015663 , -0.06976324,  0.1015663 ,  0.07784515,  1.19073666,\n",
       "        0.85124217,  1.02257171,  0.1015663 , -0.06976324,  0.1015663 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=1.0, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators=3, learning_rate = 1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=39,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators  = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0;\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_going_up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
