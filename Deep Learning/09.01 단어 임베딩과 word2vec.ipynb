{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "af2546eaeb714d609bfd191e53b75fd0"
   },
   "source": [
    "# 단어 임베딩과 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d7b18a950b864118948e4cc1511051f6"
   },
   "source": [
    "단어 임베딩(Word Embedding)이란 텍스트를 구성하는 하나의 단어를 수치화하는 방법의 일종이다.\n",
    "\n",
    "텍스트 분석에서 흔히 사용하는 방식은 단어 하나에 인덱스 정수를 할당하는 Bag of Words 방법이다. 이 방법을 사용하면 문서는 단어장에 있는 단어의 갯수와 같은 크기의 벡터가 되고 단어장의 각 단어가 그 문서에 나온 횟수만큼 벡터의 인덱스 위치의 숫자를 증가시킨다.\n",
    "\n",
    "즉 단어장이 \"I\", \"am\", \"a\", \"boy\", \"girl\" 다섯개의 단어로 이루어진 경우 각 단어에 다음과 같이 숫자를 할당한다.\n",
    "\n",
    "```\n",
    "\"I\": 0\n",
    "\"am\": 1\n",
    "\"a\": 2\n",
    "\"boy\": 3 \n",
    "\"girl\": 4\n",
    "```\n",
    "\n",
    "이 때 \"I am a girl\" 이라는 문서는 다음과 같이 벡터로 만들 수 있다.\n",
    "\n",
    "$$ [1 \\; 1 \\; 1 \\; 0 \\; 1] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "f2cf17f8158440709f180c02de520974"
   },
   "source": [
    "단어 임베딩은 하나의 단어를 하나의 인덱스 정수가 아니라 실수 벡터로 나타낸다. 예를 들어 2차원 임베딩을 하는 경우 다음과 같은 숫자 벡터가 될 수 있다.\n",
    "\n",
    "```\n",
    "\"I\": (0.3, 0.2)\n",
    "\"am\": (0.1, 0.8)\n",
    "\"a\": (0.5, 0.6)\n",
    "\"boy\": (0.2, 0.9) \n",
    "\"girl\": (0.4, 0.7)\n",
    "```\n",
    "\n",
    "단어 임베딩이 된 경우에는 각 단어 벡터를 합치거나(concatenation) 더하는(averaging, normalized Bag of Words) 방식으로 전체 문서의 벡터 표현을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e61573e85a2b49099701aeecbd864258"
   },
   "source": [
    "## Feed-Forward 신경망 언어 모형 (Neural Net Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "2fc1886688fe4817ac27ffd33a127fb4"
   },
   "source": [
    "이러한 단어 임베딩은 신경망을 이용하여 언어 모형을 만들려는 시도에서 나왔다. 자세한 내용은 다음 논문을 참고한다.\n",
    "\n",
    "* \"A Neural Probabilistic Language Model\", Bengio, et al. 2003\n",
    "  * http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "\n",
    "* \"Efficient Estimation of Word Representations in Vector Space\", Mikolov, et al. 2013\n",
    "  * https://arxiv.org/pdf/1301.3781v3.pdf\n",
    "  \n",
    "* \"word2vec Parameter Learning Explained\", Xin Rong, \n",
    "   * http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf\n",
    "\n",
    "V개의 단어를 가지는 단어장이 있을 때, 단어를 BOW 방식으로 크기 V인 벡터로 만든 다음 다음 그림과 같이 하나의 은닉층(Hidden Layer)을 가지는 신경망을 사용하여 특정 단어 열(word sequence)이 주어졌을 때 다음에 나올 단어를 예측하는 문제를 생각해 보자. 입력과 출력은 모두 BOW 방식으로 인코딩되어 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9c7yQbZibnr",
    "school_cell_uuid": "c725241457b54196a3a73152bd34b0ad"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/d0cd427eb0444148853a4bbc3fa2ac8c.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c8feff80c19746318ec3849c3b4d60e1",
    "sidetitle": true
   },
   "source": [
    "그림 20.5.1 : 신경망 언어 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "95d6218d803241109b93647714081d4c"
   },
   "source": [
    "입력 $x$가 들어가면 입력 가중치 행렬 $W$이 곱해져서 은닉층 벡터 $h$가 되는데 $x$가 one-hot-encoding 된 값이므로 $h$ 벡터는 입력 가중치 행렬  $W$의 행 하나 $w_i$에 대응된다. \n",
    "\n",
    "$$ h = \\sigma(W x) $$\n",
    "\n",
    "여기에서 $i$는 입력 벡터 $x$ 의 값이 1인 원소의 인덱스이다. 즉, BOW 단어장에서 $i$번째 단어를 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5d18b6d207d14acebe7834ba632d94da"
   },
   "source": [
    "이 $w_i$ 벡터 값을 해당 단어에 대한 **분산 표현 (distributed representation)** , **벡터 표현 (vector representation)** 또는 **단어 임베딩 (word embedding)**이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9c7yQbZibnr",
    "school_cell_uuid": "8b7e6e6d03b0453ab72d52217bb52597"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/df7c2172efdf436b97e9de5ddf47d0de.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "896966f87c264e0eb40ca91bb193b522",
    "sidetitle": true
   },
   "source": [
    "그림 20.5.2 : 단어임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "9fcfb718f20f4dec98d7a4fffb2061f0"
   },
   "source": [
    "## CBOW (Continuous Bag of Words) Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "639f85f1ddb541d7a10bc888c12c1745"
   },
   "source": [
    "위의 방식은 하나의 단어로부터 다음에 오는 단어를 예측하는 문제였다. 이러한 문제를 단어 하나짜리 문맥(single-word context)를 가진다고 한다. \n",
    "\n",
    "CBOW (Continuous Bag of Words) 방식은 복수 단어 문맥(multi-word context)에 대한 문제 즉, 여러개의 단어를 나열한 뒤 이와 관련된 단어를 추정하는 문제이다. 즉, 문자에서 나오는 $n$개의 단어 열로부터 다음 단어를 예측하는 문제가 된다. 예를 들어\n",
    "\n",
    "> the quick brown fox jumped over the lazy dog\n",
    "\n",
    "라는 문장에서 (`the`, `quick`, `brown`) 이라는 문맥이 주어지면 `fox`라는 단어를 예측해야 한다.\n",
    "\n",
    "CBOW는 다음과 같은 신경망 구조를 가진다. 여기에서 각 문맥 단어를 은닉층으로 투사하는 가중치 행렬은 모든 단어에 대해 공통으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9c7yQbZibnr",
    "school_cell_uuid": "c390639b0eba41cca95247778dd77f7d"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/e62aadf1e8324d16a66288f2c83c470a.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "6f0d45a40cbf4e298013ff9a6b613de5",
    "sidetitle": true
   },
   "source": [
    "그림 20.5.3 : CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9c7yQbZibnr",
    "school_cell_uuid": "3f2de15610604ba79996006134c4b049"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/1b6796e90e824dfe82910f0bb3ce47d5.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e13e1fbd6c5342b6a755cf25ea51480a",
    "sidetitle": true
   },
   "source": [
    "그림 20.5.4 : CBOW 임베딩 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "a3233d93b0a74b1eb8829f28cfac64b8"
   },
   "source": [
    "## Skip-Gram Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "3f8d3ef986f9418e86d5b40e6b27ffaf"
   },
   "source": [
    "Skip-Gram 방식은 CBOW 방식과 반대로 특정한 단어로부터 문맥이 될 수 있는 단어를 예측한다. 보통 입력 단어 주변의 $k$개 단어를 문맥으로 보고 예측 모형을 만드는데 이 $k$ 값을 window size 라고 한다.\n",
    "\n",
    "위 문장에서 window size $k=1$인 경우,\n",
    "\n",
    "* `quick` -> `the`\n",
    "* `quick` -> `brown`\n",
    "* `brown` -> `quick`\n",
    "* `brown` -> `fox`\n",
    "\n",
    "과 같은 관계를 예측할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9c7yQbZibnr",
    "school_cell_uuid": "e11d8a916e7043ad961f8fc8edd682bf"
   },
   "source": [
    "<img src=\"https://datascienceschool.net/upfiles/de649c0d600f410dacf09f71639209ed.png\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "c45ae91449b949eaa9bdce258b264342",
    "sidetitle": true
   },
   "source": [
    "그림 20.5.5 : Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "efc5236e08e0402a891fbbd2be0dcdaf"
   },
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "8b3a965a5c4844a092d2743e250ed402"
   },
   "source": [
    "word2vec은 CBOW 방식과 Skip-Gram 방식의 단어 임베딩을 구현한 C++ 라이브러리로 구글에 있던 Mikolov 등이 개발하였다. 이 라이브러리는 기본적인 임베딩 모형에 subsampling, negative sampling 등의 기법을 추가하여 학습 속도를 향상하였다. 파이썬에서는 gensim이라는 패키지에 `Word2Vec`이라는 클래스로 구현되어 있다. \n",
    "\n",
    "nltk의 영화 감상 corpus를 기반으로 `Word2Vec` 사용법을 살펴보자.\n",
    "\n",
    "우선 단어 임베딩을 위한 코퍼스를 만든다. 코퍼스는 리스트의 리스트 형태로 구현되어야 한다. 내부 리스트는 하나의 문장을 이루는 단어 열이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "school_cell_uuid": "58d51d64722a4c08ad8cd94a4cf1bdc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/dockeruser/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "school_cell_uuid": "aee9c11e539d4c42aacdc2b5f97fee24"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(s) for s in movie_reviews.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "school_cell_uuid": "080fb5c706ed4bdb960005490b58b581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "5e42fec8d1d24de38a2be451905b0912"
   },
   "source": [
    "다음으로 이 코퍼스를 입력 인수로 하여 Word2Vec 클래스 객체를 생성한다. 이 시점에 트레이닝이 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "school_cell_uuid": "5ff45ffa1fd64cb4b33f835a97db93b9"
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "school_cell_uuid": "aa38bb935f634e6fa7cf6f3c1d1f24bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.6 s, sys: 1.89 s, total: 42.5 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "06100fecf7df4f3ea6f82604179ef7b9"
   },
   "source": [
    "트레이닝이 완료되면 `init_sims` 명령으로 필요없는 메모리를 unload 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "school_cell_uuid": "42d399a46c9e475f8db32b6733808587"
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "e7bd1b3ac6c1447a9fe3da7ee28e7108"
   },
   "source": [
    "이제 이 모형에서 다음과 같은 메서드를 사용할 수 있다. 보다 자세한 내용은 https://radimrehurek.com/gensim/models/word2vec.html 를 참조한다.\n",
    "\n",
    "* `similarity` : 두 단어의 유사도 계산\n",
    "* `most_similar` : 가장 유사한 단어를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "school_cell_uuid": "b9e1191253f841b089c1466c823d08fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800922413447049"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('actor', 'actress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "school_cell_uuid": "7454de6adad842a88fd14d99b3835b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8576631643407555"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('he', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "school_cell_uuid": "c0fb0c18d40846cc9f7d343e9ff0c737"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2787458832170815"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('actor', 'she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "school_cell_uuid": "0121ff0a224042228159cb5bc2c46bb8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plane', 0.8854207992553711),\n",
       " ('building', 0.8792600035667419),\n",
       " ('boat', 0.8749004602432251),\n",
       " ('bed', 0.8656581044197083),\n",
       " ('wreck', 0.8589385747909546),\n",
       " ('abandoned', 0.8570672869682312),\n",
       " ('truck', 0.8565370440483093),\n",
       " ('marriage', 0.8555788993835449),\n",
       " ('island', 0.8536498546600342),\n",
       " ('apartment', 0.852118194103241)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"accident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "eee28eee971644419e234be941223180"
   },
   "source": [
    "`most_similar` 메서드는 `positive` 인수와 `negative` 인수를 사용하여 다음과 같은 단어간 관계도 찾을 수 있다.\n",
    "\n",
    "> she + (actor - actress)  = he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "school_cell_uuid": "59e5ef54b5834576840005a621b3dae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.27924999594688416)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['she', 'actor'], negative='actress', topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "school_cell_uuid": "a8c6ae5c7e8c441c8575caf4aa51827d"
   },
   "source": [
    "이번에는 네이버 영화 감상 코퍼스를 사용하여 한국어 단어 임베딩을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "school_cell_uuid": "926bf5eecd2e45b5a097bb6b9ab588fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘ratings_train.txt’ already there; not retrieving.\n",
      "\n",
      "CPU times: user 60 ms, sys: 130 ms, total: 190 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "!wget - nc https: // raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "school_cell_uuid": "c8cc4e084e514788bec407f7e93e5345"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_data('ratings_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "school_cell_uuid": "ba67db28bd3044d4b58f6a6da85a10ad"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "tagger = Twitter()\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "\n",
    "train_docs = [row[1] for row in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "school_cell_uuid": "f2850e6717134e8c94573ab3bead87fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 59s, sys: 21.3 s, total: 10min 20s\n",
      "Wall time: 11min 18s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "\n",
    "sentences = [tokenize(d) for d in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "school_cell_uuid": "84e01303b3834f798a0fb20398903c8e"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "school_cell_uuid": "377c933373164127845eb435e7ba680b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 3.48 s, total: 1min 6s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "\n",
    "model = word2vec.Word2Vec(sentences)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "school_cell_uuid": "f2dcf3a45e394db286a38002cbb805f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6978384955534069"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(*tokenize(u'배우 여배우'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "school_cell_uuid": "023a6ea6ae654987829afe9b462883e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2592131576746955"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(*tokenize(u'배우 남자'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "eee28eee971644419e234be941223180"
   },
   "source": [
    "> 남자 + (여배우 - 배우)  = 여자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "school_cell_uuid": "06b4407cf7364fef9ec5f8b1986ea174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('여자/Noun', 0.8009182214736938)]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.utils import pprint\n",
    "pprint(model.wv.most_similar(positive=tokenize(\n",
    "    u'남자 여배우'), negative=tokenize(u'배우'), topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "school_cell_uuid": "d707cf8a56174171a919326b5c3f05eb"
   },
   "source": [
    "더 많은 한국어 코퍼스를 사용한 단어 임베딩 모형은 다음 웹사이트에서 테스트해 볼 수 있다.\n",
    "* http://w.elnn.kr/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}